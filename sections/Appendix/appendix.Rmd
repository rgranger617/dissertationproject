---
title: ''
output: pdf_document
---

<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%% Appendix A %%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->

\section{Appendix A: Conditional Maximum Likelihood Estimation}

\label{Sec:conditionalmaximumlike}

Instead of using the full likelihood we replace it with the conditional likelihood, conditioned on each individual being observed once. 

\begin{align}
\nonumber p(\mathcal{Y}|N,\boldsymbol{\beta},\mathcal{X}_{obs},\mathcal{X}_{mis};\boldsymbol{y}\ne \boldsymbol{0}) = & \frac{p(\mathcal{Y}|N,\boldsymbol{\beta},\mathcal{X}_{obs},\mathcal{X}_{mis})}{P(\boldsymbol{y}\ne \boldsymbol{0})} \\
= & \prod_{i=1}^N \frac{\prod_{j=1}^J \lambda_{ij}^{y_{ij}}(1-\lambda_{ij})^{1-y_{ij}}}{1-\prod_{j=1}^J(1-\lambda_{ij})}
\end{align}

The posterior distribution can therefore be written as

\begin{align}
\label{eqn:condposteriorderivevalues}
\nonumber p(N,\boldsymbol{\beta},\mathcal{X}_{mis}|\mathcal{Y},\mathcal{X}_{obs}) \propto &  \left[\prod_{i=1}^N \frac{\prod_{j=1}^J \lambda_{ij}^{y_{ij}}(1-\lambda_{ij})^{1-y_{ij}}}{1-\prod_{j=1}^J(1-\lambda_{ij})} \right] \times \left[ \prod_{i=1}^N \boldsymbol{g}(\boldsymbol{\theta_h}) \right] \times  \left[\frac{1}{N}\right] \\
\times & \left[ \prod_{j=1}^J \left(\frac{1}{2\pi}\right)^{H/2}|B|^{-1/2}e^{-\frac{1}{2}(\boldsymbol{b}-\boldsymbol{\beta_j})^T\boldsymbol{B}^{-1}(\boldsymbol{b}-\boldsymbol{\beta_j})}\right].
\end{align}

Instead of computing a MCMC sampler, we will instead compute the Maximum a posteriori (MAP) estimate for the coefficients, $\boldsymbol{\beta}_{MAP}$, using gradient ascent.  Then, this estimate can be plugged into a Horvitz-Thompson estimator to find the $N_{MAP}$.

Taking into consideration only the parts of the posterior that depend on $\boldsymbol{\beta}$, the log posterior is
\begin{align}
\nonumber \ln p(N,\boldsymbol{\beta},\mathcal{X}_{mis}|\mathcal{Y},\mathcal{X}_{obs}) \propto & \sum_{i=1}^N \Big( \sum_{j=1}^J \Big( y_{ij}\ln(\lambda_{ij}) + (1-y_{ij})\ln(1-\lambda_{ij})\Big) 
- \ln(1-\prod_{j=1}^J(1-\lambda_{ij}))\Big) \\
\nonumber & -\frac{1}{2}\sum_{j=1}^J (b-\boldsymbol{\beta_j})^T B^{-1}(b-\boldsymbol{\beta_j}) 
\end{align}

Taking the first derivative with respect to $\boldsymbol{\beta_j}$ gives the gradient

\begin{align}
\frac{\Delta \ln p(N,\boldsymbol{\beta},\mathcal{X}_{mis}|\mathcal{Y},\mathcal{X}_{obs})}{\Delta \boldsymbol{\beta_j}} = & \hspace{1px} B^{-1}(b-\boldsymbol{\beta_j}) + \sum_{i=1}^n \left(y_{ij}-\lambda_{ij}+\frac{\lambda_{ij}\prod_{j=1}^J(1-\lambda_{ij})}{1-\prod_{j=1}^J(1-\lambda_{ij})} \right)\boldsymbol{x_i}^T.
\end{align}

Notice in the equation above that only the observed data appears.  This convenience occurs due to the fact that E[$y_{ij}|\boldsymbol{y_i}= \boldsymbol{0}$] = 0 for all individuals \citep{alho_logistic_1990}.  Using the gradient, apply the gradient ascent algorithm until convergence to obtain $\boldsymbol{\beta}_{MAP}$.  

\newpage

<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%% Appendix B %%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->

\section{Appendix B: LCMCR Model}
\label{LCMCRmodel}

This section summarizes the Bayesian Non-Parametric Latent-Class Capture-Recapture (LCMCR) derivation and explanations provided in \cite{manriquevallier_bayesian_2016}.
The LCMCR model is a framework for the capture-recapture (CR) problem that uses the Bayesian nonparametric latent-class model (NPLCM) proposed in \cite{dunson_nonparametric_2009} to model $f(\boldsymbol{y_i}|\theta)$.

In order to account for unobserved heterogeneity, each individual, $i$, is modeled such that they belong to a hidden, latent class, $z_i$, with probability $\pi_k$.  After determining a latent class, an individual is captured according to a Bernoulli distribution on list $j$ with probability, $\lambda_{jk}$. This is known as the latent-class model \citep{goodman_exploratory_1974}, and yields the probability mass function

\begin{equation}
\label{eqn:bernmix}
f(\boldsymbol{y_i}|\boldsymbol{\lambda,\pi}) = \sum_{k=1}^K \pi_k \prod_{j=1}^J \lambda_{jk}^{y_{ij}}(1-\lambda_{jk})^{1-y_{ij}},
\end{equation}

where $\boldsymbol{\pi}=(\pi_1,...,\pi_K)$ and $\boldsymbol{\lambda} = (\lambda_{jk})$ for $j=1,...,J$ and $k=1,...,K$.

Inserting \autoref{eqn:bernmix} into \autoref{eqn:jointlikelihoodequation1} yields the following likelihood equation

\begin{equation}
\label{eqn:LCMCRlikelihood}
p(\mathcal{Y}|\boldsymbol{\lambda},\boldsymbol{\pi},N)=\binom{N}{n} \left[ \sum_{k=1}^K \pi_k \prod_{j=1}^J (1-\lambda_{jk}) \right]^{N-n} \times \prod_{i=1}^n \sum_{k=1}^K \pi_k \prod_{j=1}^J \lambda_{jk}^{y_{ij}}(1-\lambda_{jk})^{1-y_{ij}}.
\end{equation}

The number of latent classes is endogenized with the probability of belonging to each latent class, $\pi_k$, being drawn from a "stick-breaking" process \citep{sethuraman_constructive_1991}.  The parameter, $\alpha$, controls the amount of concentration of the probability mass.  In other words, larger values of $\alpha$ will lead to a larger number of relevant latent classes.  A Beta(1,1) prior distribution is placed on each of the $J \times K^*$ probabilities, and a Gamma($a,b$) prior is placed on $\alpha$.

The model can be summarized through the following hierarchical generative process

\begin{align} 
\nonumber y_{ij}|z_i & \sim \text{Bernoulli}(\lambda_{jz}) \hspace{10px} \text{for } j=1,...,J \text{ and } i=1,...,N\\  \nonumber
z_i & \sim \text{Discrete}(\{1,2,...\},(\pi_1,\pi_2,...)) \hspace{10px} \text{for } i=1,...,N \\ \nonumber
\lambda_{jk} & \sim \text{Beta}(1,1) \hspace{10px} \text{for } j=1,...,J \text{ and } k=1,2,... \\ \nonumber
(\pi_1,\pi_2,...) & \sim \text{SB}(\alpha) \\ 
\alpha & \sim \text{Gamma}(a,b).
\end{align}

<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%% Appendix C %%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->

\newpage

