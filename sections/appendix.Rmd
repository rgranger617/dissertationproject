---
title: ''
output: pdf_document
---

\begin{appendices}

\section{Conditional Maximum Likelihood Estimation}

\label{Sec:conditionalmaximumlike}

Instead of using the full likelihood we replace it with the conditional likelihood, conditioned on each individual being observed once. 

\begin{align}
\nonumber p(\mathcal{Y}|N,\boldsymbol{\beta},\mathcal{X}_{obs},\mathcal{X}_{mis};\boldsymbol{y}\ne \boldsymbol{0}) = & \frac{p(\mathcal{Y}|N,\boldsymbol{\beta},\mathcal{X}_{obs},\mathcal{X}_{mis})}{P(\boldsymbol{y}\ne \boldsymbol{0})} \\
= & \prod_{i=1}^N \frac{\prod_{j=1}^J \lambda_{ij}^{y_{ij}}(1-\lambda_{ij})^{1-y_{ij}}}{1-\prod_{j=1}^J(1-\lambda_{ij})}
\end{align}

The posterior distribution can therefore be written as

\begin{align}
\label{eqn:condposteriorderivevalues}
\nonumber p(N,\boldsymbol{\beta},\mathcal{X}_{mis}|\mathcal{Y},\mathcal{X}_{obs}) \propto &  \left[\prod_{i=1}^N \frac{\prod_{j=1}^J \lambda_{ij}^{y_{ij}}(1-\lambda_{ij})^{1-y_{ij}}}{1-\prod_{j=1}^J(1-\lambda_{ij})} \right] \times \left[ \prod_{i=1}^N \boldsymbol{g}(\boldsymbol{\theta_h}) \right] \times  \left[\frac{1}{N}\right] \\
\times & \left[ \prod_{j=1}^J \left(\frac{1}{2\pi}\right)^{H/2}|B|^{-1/2}e^{-\frac{1}{2}(\boldsymbol{b}-\boldsymbol{\beta_j})^T\boldsymbol{B}^{-1}(\boldsymbol{b}-\boldsymbol{\beta_j})}\right].
\end{align}

Instead of computing a MCMC sampler, we will instead compute the Maximum a posteriori (MAP) estimate for the coefficients, $\boldsymbol{\beta}_{MAP}$, using gradient ascent.  Then, this estimate can be plugged into a Horvitz-Thompson estimator to find the $N_{MAP}$.

Taking into consideration only the parts of the posterior that depend on $\boldsymbol{\beta}$, the log posterior is
\begin{align}
\nonumber \ln p(N,\boldsymbol{\beta},\mathcal{X}_{mis}|\mathcal{Y},\mathcal{X}_{obs}) \propto & -\frac{1}{2}\sum_{j=1}^J (b-\boldsymbol{\beta_j})^T B^{-1}(b-\boldsymbol{\beta_j}) \\
\nonumber + & \sum_{i=1}^N \sum_{j=1}^J y_{ij}\ln(\lambda_{ij}) + (1-y_{ij})\ln(1-\lambda_{ij}) \\ 
+  & \ln(1-\prod_{j=1}^J(1-\lambda_{ij}))
\end{align}

Taking the first derivative with respect to $\boldsymbol{\beta_j}$ gives the gradient

\begin{align}
\frac{\Delta \ln p(N,\boldsymbol{\beta},\mathcal{X}_{mis}|\mathcal{Y},\mathcal{X}_{obs})}{\Delta \boldsymbol{\beta_j}} = & \hspace{1px} B^{-1}(b-\boldsymbol{\beta_j}) + \sum_{i=1}^n \left(y_{ij}-\lambda_{ij}+\frac{\lambda_{ij}\prod_{j=1}^J(1-\lambda_{ij})}{1-\prod_{j=1}^J(1-\lambda_{ij})} \right)\boldsymbol{x_i}^T.
\end{align}

Notice in the equation above that only the observed data appears.  This convenience occurs due to the fact that E[$y_{ij}|\boldsymbol{y_i}= \boldsymbol{0}$] = 0 for all individuals \citep{alho_logistic_1990}.  Using the gradient, apply the gradient ascent algorithm until convergence to obtain $\boldsymbol{\beta}_{MAP}$.  


\section{Dirichlet Process of Mixture Normals}
\label{DPnormalmix}

The generative process for each observation's covariate is,

\begin{align}
\boldsymbol{x_i}|z_i \stackrel{ind}{\sim} & \text{ MVNormal}(\boldsymbol{\mu_k},\boldsymbol{\Sigma_k}) \hspace{5px} \text{for } i=1,...,N\\
z_i \stackrel{iid}{\sim} & \text{ Discrete}(\{1,2,...\},(\pi_1,\pi_2,...)) \hspace{5px} \text{for } i=1,...,N.
\end{align}

The probability density function of the covariates can be formally written as,

\begin{equation}
g(\boldsymbol{x}|\boldsymbol{\mu},\boldsymbol{\Sigma}) = \prod_{i=1}^N \prod_{k=1}^\infty \pi_k \text{ MVNormal}(\boldsymbol{x_i}|\boldsymbol{\mu_k},\boldsymbol{\Sigma_k})
\end{equation}

In order to have a fully Bayesian approach, we assign priors to the unknown parameters.

\begin{align}
\boldsymbol{\Sigma_k} \sim & \text{InvWishart}(\nu_{0},\boldsymbol{\Lambda_{0}^{-1}}) \\
\boldsymbol{\mu_k}|\boldsymbol{\Sigma_k} \sim & \text{ MVNormal}(\boldsymbol{\mu_{0}},\boldsymbol{\Sigma_k}/\kappa_{0}) \\
(\pi_1,\pi_2,...) \sim & SB(\alpha) \\
\alpha \sim & \text{ Gamma}(a_{\alpha},b_{\beta}),
\end{align}

where SB($\alpha$) is the stick breaking process \citep{dunson_nonparametric_2009}. While the problem is defined as having an infinite number of mixture components, we solve the finite-dimension problem with the number of mixtures truncated at an upper bound, $K^*$.  Because the stick breaking process orders the number of mixtures, as long as $K^*$ is set sufficently large, this construction approximates the infinite-dimensional problem.    The stick breaking prior can therefore be defined as $\pi_k = V_k \prod_{l<k}(1-V_l)$, where $V_1, ..., V_{K^*-1} \sim \text{Beta}(1,\alpha)$ and $V_{K^*}=1$.  

Using this generative scheme adds five additional sampling stages to the algorithm presented in section \ref{sec:CRwithCovariates}.

\begin{list}{}{}

\item[1)] Sample $z_i$ for $i=1,...,N$. The latent class label takes integer values from $1,...,K^*$.  To compute the probability of each latent class label for each $i$,

\begin{equation}
P(z_i=k) = \frac{\pi_k \text{ MVNormal}(\boldsymbol{x_i}|\boldsymbol{\mu_k},\boldsymbol{\Sigma_k})}{\sum_{l=1}^{K^*}\pi_l \text{ MVNormal}(\boldsymbol{x_i}|\boldsymbol{\mu_l},\boldsymbol{\Sigma_l})}.
\end{equation}

\item[2)] Sample $\boldsymbol{\Sigma_k}$ for $k=1,...,K^*$. Define $N_k=\sum_{i=1}1_{z_i=k}$ which is a count of the number of individuals in the population belonging to latent class, $k$.  Also, define the sufficient statistics, $\boldsymbol{\bar{x}_k}=\frac{1}{N_k}\sum_{i=1}^{N_k}\boldsymbol{x_{ik}}$ and $\boldsymbol{S_k} = \sum_{i=1}^{N_k}(\boldsymbol{x_{ik}}-\boldsymbol{\bar{x}_k})(\boldsymbol{x_{ik}}-\boldsymbol{\bar{x}_k})^T$.  Then,

\begin{equation}
\boldsymbol{\Sigma_k} \sim \text{InvWishart}(\nu_{N_k},\boldsymbol{\Lambda_{N_k}^{-1}}),
\end{equation}

where $\nu_{N_k} = \nu_{0}+N_k$ and $\boldsymbol{\Lambda_{N_k}}=\boldsymbol{\Lambda_{0}}+\boldsymbol{S_k}+\frac{\kappa_{0}N_k}{\kappa_{0}+N_k}(\boldsymbol{\bar{x}_k}-\boldsymbol{\mu_{0}})(\boldsymbol{\bar{x}_k}-\boldsymbol{\mu_{0}})^T$.

\item[3)] Sample $\boldsymbol{\mu_k|\Sigma_k}$ for $k=1,...,K^*$.  Using the same defined terms in the previous step,

\begin{equation}
\boldsymbol{\mu_k}|\boldsymbol{\Sigma_k} \sim \text{MVNormal}(\boldsymbol{\mu_{N_k}},\boldsymbol{\Sigma_k}/\kappa_{N_k} ),
\end{equation}

where $\boldsymbol{\mu_{N_k}}=\frac{\kappa_{0}}{\kappa_{0}+N_k}\boldsymbol{\mu_{0}} + \frac{N_k}{\kappa_{0}+N_k}\boldsymbol{\bar{x}_k} $ and $\kappa_{N_k}=\kappa_{0}+N_k$.

\item[4)] Sample $(\pi_1,\pi_2,...,\pi_{K^*})$ for $k=1,...,K^*$.  Begin by drawing a sample from each of the stochastic components,

\begin{equation}
 V_k \sim \text{ Beta}\left(1+N_k, \alpha+\sum_{l=k+1}^K N_l\right) \hspace{5px} \text{for } k=1,...,K^*-1.
\end{equation}

Set $V_{K*}=1$.  Then, $\pi_k = V_k\prod_{l<k}(1-V_l)$ for all $k=1,...,K^*$.

\item[5)] Sample $\alpha$.

\begin{equation}
  \alpha \sim \text{ Gamma}\left(a_\alpha + K^* -1, b_\alpha - \ln(\pi_{K^*}) \right).
\end{equation}

\end{list}


\section{LCMCR Model}
\label{LCMCRmodel}

This section summarizes the Bayesian Non-Parametric Latent-Class Capture-Recapture (LCMCR) derivation and explanations provided in \cite{manriquevallier_bayesian_2016}.
The LCMCR model is a framework for the capture-recapture (CR) problem that uses the Bayesian nonparametric latent-class model (NPLCM) proposed in \cite{dunson_nonparametric_2009} to model $f(\boldsymbol{y_i}|\theta)$.

In order to account for unobserved heterogeneity, each individual, $i$, is modeled such that they belong to a hidden, latent class, $z_i$, with probability $\pi_k$.  After determining a latent class, an individual is captured according to a Bernoulli distribution on list $j$ with probability, $\lambda_{jk}$. This is known as the latent-class model \citep{goodman_exploratory_1974}, and yields the probability mass function

\begin{equation}
\label{eqn:bernmix}
f(\boldsymbol{y_i}|\boldsymbol{\lambda,\pi}) = \sum_{k=1}^K \pi_k \prod_{j=1}^J \lambda_{jk}^{y_{ij}}(1-\lambda_{jk})^{1-y_{ij}},
\end{equation}

where $\boldsymbol{\pi}=(\pi_1,...,\pi_K)$ and $\boldsymbol{\lambda} = (\lambda_{jk})$ for $j=1,...,J$ and $k=1,...,K$.

Inserting \autoref{eqn:bernmix} into \autoref{eqn:jointlikelihoodequation1} yields the following likelihood equation

\begin{equation}
\label{eqn:LCMCRlikelihood}
p(\mathcal{Y}|\boldsymbol{\lambda},\boldsymbol{\pi},N)={N\choose n} \left[ \sum_{k=1}^K \pi_k \prod_{j=1}^J (1-\lambda_{jk}) \right]^{N-n} \times \prod_{i=1}^n \sum_{k=1}^K \pi_k \prod_{j=1}^J \lambda_{jk}^{y_{ij}}(1-\lambda_{jk})^{1-y_{ij}}.
\end{equation}

The number of latent classes is endogenized with the probability of belonging to each latent class, $\pi_k$, being drawn from a "stick-breaking" process \citep{sethuraman_constructive_1991}.  The parameter, $\alpha$, controls the amount of concentration of the probability mass.  In other words, larger values of $\alpha$ will lead to a larger number of relevant latent classes.  A Beta(1,1) prior distribution is placed on each of the $J \times K^*$ probabilities, and a Gamma($a,b$) prior is placed on $\alpha$.

The model can be summarized through the following hierarchical generative process

\begin{align} 
\nonumber y_{ij}|z_i & \sim \text{Bernoulli}(\lambda_{jz}) \hspace{10px} \text{for } j=1,...,J \text{ and } i=1,...,N\\  \nonumber
z_i & \sim \text{Discrete}(\{1,2,...\},(\pi_1,\pi_2,...)) \hspace{10px} \text{for } i=1,...,N \\ \nonumber
\lambda_{jk} & \sim \text{Beta}(1,1) \hspace{10px} \text{for } j=1,...,J \text{ and } k=1,2,... \\ \nonumber
(\pi_1,\pi_2,...) & \sim \text{SB}(\alpha) \\ 
\alpha & \sim \text{Gamma}(a,b).
\end{align}


\end{appendices}