---
title: ''
output: pdf_document
---
\newpage
\section{Literature Review}
\label{Sec:litreview}

Capture-Recapture (CR) refers to a series of methods that are used to estimate the size of a population from at least two incomplete, matched lists.  This naming comes from the process of first capturing/marking a number of organisms (capture) and following up with a second capturing occasion (recapture). One then estimates the population size, $N$, of the organism by comparing the number of organisms marked in each sample, $n_1$ and $n_2$, and the organisms that were matched in both samples, $n_{12}$.  Intuitively, the proportion of the total population that is initially captured/marked, $n_1/N$, should be equal to the proportion of captured/marked individuals that are recaptured, $n_{12}/n_2$. Of course, this only makes sense if we believe the lists are independent. Rearranging and solving for $N$ leads to the formula:

$$\hat{N} = \frac{n_1 \cdot n_2}{n_{12}}.  $$

Although this estimator was used as early as 1783 by the mathematician Pierre-Simon Laplace for the purpose of estimating the French population \citep{schaefer_estimation_1951}, this estimator is commonly referred to as the Lincoln-Petersen estimator after its early use by Frederick C. Lincoln on migratory waterfowl \citep{lincoln_calculating_1930} and C.G. Johannes Petersen on the fish species plaice \citep{petersen_yearly_1895}.  The Lincoln-Petersen estimator is relatively simple to compute, but is limited to only two lists with the assumption of a closed population.  A closed population means the population does not change throughout the sampling occasions.  This estimator also makes two other intricately related assumptions:  

\begin{list}{}{}

\item[1)] independence between lists,

\item[2)] homogeneity in capture probabilities between individuals.

\end{list}

The first assumption of independence between lists means the probability an individual is captured on one list does not dependent on the probability that individual is captured on another list.  The second assumption means the probability of any two individuals being captured on a list is the same. When these assumptions are violated, it can lead to bias in our population estimates. Addressing violations of these assumptions has a long history and continues to be looked into today.

<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%% Section on Early Aproaches %%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->

\subsection{Early Approaches to List Dependency and Homogeneity}
\label{sec:earlyapproachlitreview}

Capture-recapture solutions with multiple lists appear at least as early as the 1920s \citep{geiger_zahl_1924,schnabel_estimation_1938,darroch_multiple-recapture_1958}.  All use a model for estimating the size of a population from multiple lists with list independence.  If this assumption were to hold, then the probability an individual appears on one list would not impact the probability that the individual appears on another list. Perhaps for carefully controlled experiments, list independence can be assumed, but for most real applications, this is unlikely. For example with human rights data, this may be violated as psychological research indicates survivors of human rights abuses can benefit in their mental health by sharing their testimony.  These individuals may in turn be encouraged to share their experiences to multiple outlets. Furthermore, there is evidence with the Guatemalan Civil War that several popular movements groups encouraged their social bases to provide testimony of human rights abuses to all of three of the data collecting organizations \citep{ball_making_2000}. These conditions lead to positive list dependence and therefore an underestimate of the population size. On the other hand, there is also the potential that certain groups of individuals may feel uneasy sharing their testimony with non-aligned data collection groups, leading to negative dependence and an overestimate of the population size.  For example, religious members of the Catholic faith were more apt to give their testimony to the Catholic researchers than they were to the political left, non-governmental organizations \citep{manrique-vallier_capture-recapture_2020}. 

In the early 1970s, two approaches were developed to address list dependency. \cite{fienberg_multiple_1972} addressed the issue of list dependency directly by modeling interactions between lists using hierarchical log linear models under a multinomial sampling scheme.  On the other hand, \cite{sanathanan_models_1972} also used a multinomial sampling scheme but instead of modeling list dependence directly, she modeled it as unobserved heterogeneity in the individuals under a conditional likelihood approach. As pointed out in several places \citep{darroch_three-sample_1993,fienberg_classical_1999,manrique-vallier_capture-recapture_2020}, list dependency and heterogeneity between individuals are intricately related such that list dependency can be viewed as a result of heterogeneity between individuals. This leads to the second assumption, homogeneity, which refers to all individuals having the same capture probability within a list. For a variety of reasons, we may expect certain individuals to be more likely to be captured because of characteristics like geographic area, age, or their status within the community. 

This heterogeneity among individuals may be unobserved, observed through covariates, or a combination. In the 1990s, various methods were proposed to account for unobserved heterogeneity using adaptations to log-linear and mixture models \citep{darroch_three-sample_1993,agresti_simple_1994,pledger_unified_2000}. Later, Bayesian methods based on latent class models were presented that allowed for a more flexible approach to heterogeneity \citep{fienberg_classical_1999,manrique-vallier_population_2008,manriquevallier_bayesian_2016}. These methods seek out patterns in the capture histories in order to infer for unobserved heterogeneity.  When the researcher has accompanying discrete covariate data, a suggestion noted by \cite{darroch_three-sample_1993}, is to first stratify the data based on the observable heterogeneity \citep{sekar_method_1949}, and then perform the desired method within that stratification. An example of this this implementation can be found in \cite{manrique-vallier_estimating_2019}, where the authors estimate the number of casualties in the internal conflict in Peru (1980-2000) based on geographic stratifications. Stratification becomes problematic in the presence of continuous covariates.  When continuous covariates are present, the researcher must make difficult decisions on how to discretize the covariates which leads to assigning potentially arbitrary cutoffs.  If your stratification scheme results in too many strata, the number of observed individuals within each strata may be small, resulting in loss of inferential power and identifiability.  These issues can arise even without continuous covariates if there are many discrete variables with many unique levels.  


<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%% CR with covariates %%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->


\subsection{Capture-Recapture with Covariates}

An alternative to stratification is to use the covariates within the modeling process like with regression. \cite{pollock_use_1984} suggested the use of logistic regression with environmental and individual level covariates.  \cite{huggins_statistical_1989,alho_logistic_1990} independently proposed a similar but slightly different approach that uses conditional maximum likelihood logistic regression through a two step process. First, the coefficients for the logistic regression model are obtained by regressing whether the individual was captured against the covariates under the condition the individual is observed at least once. If we perform the regression without taking into account that only observed individuals appear in the dataset, i.e., some individuals are missing, the coefficients become biased.  After obtaining the coefficients, we fit the probability that each individual is captured on each list, and the probability an individual is missing, $\theta_i$, is computed. The second step uses the estimated probability of missing by plugging it into the \cite{horvitz_generalization_1952} estimator, 

\begin{equation*}
\hat{N} = \sum_i^n \frac{1}{\theta_i},
\end{equation*}

to obtain an estimate for $N$. Intuitively, the estimator inverts the probability of missing to account for the number of similar individuals that would have been unobserved. This intutition implies that the estimator will not perform well when the covariates for the observed individuals vary significantly from the unobserved individuals.  Nevertheless, \cite{alho_logistic_1990} shows this estimator to be consistent with large samples and develops an asymptotic approach to estimating the variance. 

In these models, the probability an individual is captured on each list is conditionally independent of the other lists conditional on the covariates. While covariates may explain some of the heterogeneity, it is unlikely that all heterogeneity between individuals can be explained in such a fashion and that some list dependency would not remain. \cite{zwane_population_2005} use multinomial logistic regression with a design matrix intended to directly model the dependence between the lists.  One could theoretically incorporate additional rows into the design matrix to account for unobserved heterogeneity as in \cite{darroch_three-sample_1993}.

Since the covariates will be missing for any of the unobserved individuals, any approach that uses the complete likelihood would require the covariate distribution to be specified.  All of the previous mentioned methods sidestep this need by maximizing the conditional likelihood instead of the full likelihood. \cite{stoklosa_heterogeneous_2011} takes this a step further by comparing a partial likelihood approach, i.e, the number of recaptures after the first capture, with the conditional approach. They find a loss of efficiency but argue that it allows more flexibility in modeling. Later, \cite{yee_vgam_2015} presented a simple way to implement conditional likelihood methods using the \texttt{VGAM} package in the programming software \texttt{R}. There is some early usage of a full likelihood approach appearing in work that combines line transact theory with capture-recapture for population estimation \citep{alpizar-jara_combination_1996,borchers_mark-recapture_1998}. Since the full likelihood approach requires specifying a distribution for the covariates, this presents the difficulty of potentially estimating additional parameters regarding the covariate distribution and/or integrating out the covariates to obtain an estimate for $N$.  While more challenging, \cite{pollock_use_2002} speculates the full likelihood approach could result in better precision and suggests a possible solution may lie with Bayesian methods.

<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%%% Bayesian CR %%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->
<!-- %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -->

\subsection{Bayesian Methods for Capture-Recapture}

Perhaps the earliest paper to utilize Bayesian methods is \cite{roberts_informative_1967} in the evaluation of "stopping rules" with exactly two samples. The "stopping rule" refers to how the sample of individuals from the population will be collected, and in particular, what criterion must be met before concluding the sampling occasion. For example, a researcher may choose a fixed sample size and will stop sampling subjects when this sample size is reached. On the other hand, the researcher may attempt to sample every subject within a fixed time period.  Depending on the chosen stopping rule, the likelihoods can differ and thus the estimator. See \cite{chapman_estimation_1954,darroch_multiple-recapture_1958} for a non-Bayesian look at the problem.  These different stopping rules were once again considered in the Bayesian approaches of \cite{castledine_bayesian_1981} and extended to include more than two lists. Further, the authors assume the probability of capture is constant between individuals but allowed to vary across time (see model $M_t$ in \cite{otis_statistical_1978}). The impact of various prior specifications is explored including the use of the Beta prior for the probability of capture and the Jeffreys prior \citep{jeffreys_theory_1967} for $N$.  Under similar modelling, estimates of the posterior distribution were obtained using empirical Bayes and Bayes empirical Bayes  \citep{smith_bayesian_1991}, and Markov Chain Monte Carlo (MCMC), Gibbs sampling, \citep{george_capture-recapture_1992} methodology.



Bayesian approaches to unobserved heterogeneity were first introduced to the multiple-recapture problem in \cite{fienberg_classical_1999}.  The authors use a fully Bayesian hierarchical approach through the use the Rasch model and develop a MCMC procedure for obtaining samples from the posterior. \cite{manrique-vallier_population_2008} also employ a fully Bayesian hierarchical approach but use a Grade of Membership model \citep{woodbury_mathematical_1978} to account for the unobserved heterogeneity.  Individuals are soft or partially clustered into classes, i.e. mixed membership, with each class having an estimated probability of capture per list. A simplification to the partial membership approach is to use the Latent Class Model (LCM) where individuals are hard clustered into groups. \citep{manriquevallier_bayesian_2016} uses a Bayesian Nonparametric Latent Class Model (NPLCM) by using a potentially infinite  number of hidden classes through a stick-breaking prior \citep{dunson_kernel_2008}. Posterior samples are obtained via a Gibbs sampler.

Naturally, when deriving a posterior distribution, one would use the full or "complete" likelihood. This has the benefit of 1) being the "correct" way to compute the posterior based on a specified generative process and 2) allows us to extend our model without fear of compounding any errors from modifications/approximations. Unfortunately, the use of a full likelihood approach in the presence of covariates poses a particularly challenging problem as a distribution for the covariates must be specified and the posterior often becomes intractable. The early approach of using the conditional likelihood instead of the full likelihood avoids this problem but recently several other solutions have been presented.  These include the use of a reversible jump MCMC \citep{king_bayesian_2008}, data augmentation \citep{royle_analysis_2007,royle_analysis_2009}, and replacing the full likelihood with a "semi-complete" data likelihood approach \citep{king_capturerecapture_2016}. The semi-complete likelihood uses the complete likelihood for the observed individuals and a marginal likelihood for the unobserved individuals. Both approaches still require estimating the probability of missing, which is a difficult integral.  While \citep{king_capturerecapture_2016} evaluate the integral using Gauss-Hermite quadrature, \cite{bonner_mcmcmc_2014} estimates the probability of missing with a Monte Carlo within MCMC step. The efficiency of this approach depends on the size of the Monte Carlo simulated covariate distribution with a larger sample giving a better approximation, but at the cost of reducing the efficiency of the algorithm.  It should be noted that whether we use a full likelihood or even a semi-complete likelihood, a covariate distribution must be specified. The examples provided often assume a normal distribution, but \cite{royle_analysis_2009} does consider alternative distributions. These alternative distributions do reveal some sensitivity to the choice of covariate distribution, which will be explored further in subsection \ref{Sec:selectcovariates}.


