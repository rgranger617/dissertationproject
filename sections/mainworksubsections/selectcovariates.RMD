---
title: ''
output: pdf_document
---

\subsection{Selecting a Distribution for the Covariates}
\label{Sec:selectcovariates}

When the probability that an individual is captured at least once is dependent upon the covariates, the observed covariate distribution will differ from population covariate distribution.  Simply using logistic regression on the observed data would lead to biased coefficients, which in turn would lead to bias in the population estimation.  The algorithm presented in subsection \ref{Sec:BLRCRmodel} alleviates this problem by concatenating samples of the missing covariates with the observed data before estimating the coefficients.  Unfortunately, this presents an additional burden on the user of specifying a distribution for the missing covariates, $\boldsymbol{g}(\boldsymbol{\phi})$. 

\subsubsection{Specifying a Distribution for the Covariates}
\label{sec:normaldistributioncovariate}

If the distribution is known including parameters, then we can simply use the aforementioned algorithm. If the distribution is known except for the parameters, then we could specify that distribution along with priors on the parameters. For example, \cite{royle_analysis_2009} uses a single covariate and specifies a normal distribution with a normal distribution prior for the mean and a gamma prior for the inverse variance. One could take a multivariate version of this approach by specifying,

\begin{align} 
\label{eqn:normalcovariatedistribution}
\boldsymbol{x_{i}} & \stackrel{iid}{\sim} \text{MVNormal}(\boldsymbol{\mu},\boldsymbol{\Sigma}),
\end{align}

with conjugate priors,

\begin{align}
\boldsymbol{\Sigma} \sim & \text{InvWishart}(\nu_{0},\boldsymbol{\Lambda_{0}^{-1}}) \\
\boldsymbol{\mu}|\boldsymbol{\Sigma} \sim & \text{ MVNormal}(\boldsymbol{\mu_{0}},\boldsymbol{\Sigma}/\kappa_{0})
\end{align}

This would add two additional sampling stages to the algorithm (see \cite{gelman_bayesian_2014}):

\begin{list}{}{}
\item[1)] Sample $\boldsymbol{\Sigma}$.  Define the sufficient statistics, $\boldsymbol{\bar{x}}=\frac{1}{N}\sum_{i=1}^{N}\boldsymbol{x_{i}}$ and $\boldsymbol{S} = \sum_{i=1}^{N}(\boldsymbol{x_{i}}-\boldsymbol{\bar{x}})(\boldsymbol{x_{i}}-\boldsymbol{\bar{x}})^T$.  Then,

\begin{equation}
\boldsymbol{\Sigma} \sim \text{InvWishart}(\nu_{N},\boldsymbol{\Lambda_{N}^{-1}}),
\end{equation}

where $\nu_{N} = \nu_{0}+N$ and $\boldsymbol{\Lambda_{N}}=\boldsymbol{\Lambda_{0}}+\boldsymbol{S}+\frac{\kappa_{0}N}{\kappa_{0}+N}(\boldsymbol{\bar{x}}-\boldsymbol{\mu_{0}})(\boldsymbol{\bar{x}}-\boldsymbol{\mu_{0}})^T$.

\item[2)] Sample $\boldsymbol{\mu_k|\Sigma_k}$ for $k=1,...,K^*$.  Using the same defined terms in the previous step,

\begin{equation}
\boldsymbol{\mu}|\boldsymbol{\Sigma} \sim \text{MVNormal}(\boldsymbol{\mu_{N}},\boldsymbol{\Sigma}/\kappa_{N} ),
\end{equation}

where $\boldsymbol{\mu_{N}}=\frac{\kappa_{0}}{\kappa_{0}+N}\boldsymbol{\mu_{0}} + \frac{N}{\kappa_{0}+N}\boldsymbol{\bar{x}} $ and $\kappa_{N}=\kappa_{0}+N$.
\end{list}

Different specified distributions would require different sampling procedures. In a supplementary document, \cite{royle_analysis_2009} considers other types of covariate distributions and found some variations in the inference. Unfortunately, knowing what distribution to specify can be difficult as rarely would we know this distribution.  Deciding on a proposal distribution based upon the observed distribution can be also dangerous or misleading as the observed distribution is a truncation of the true distribution. Not only are the missing covariates not missing at random but we do not even know how many are missing.  As we will shown in section \ref{Sec:simulations}, mispecifying the distribution can lead to inaccurate estimations.


\subsubsection{Conditional Likelihood}

One approach to this problem is to estimate the coefficients using conditional maximum likelihood, where the likelihood function to be maximized is conditioned on the probability an individual is observed at least once \citep{alho_logistic_1990,huggins_statistical_1989}. This avoids the necessity of specifying a distribution for the covariates but can lead to unstable results when the observed distribution of the covariates is dissimilar to the population distribution of the covariates \citep{tilling_capture-recapture_1999}.  An article by \cite{yee_vgam_2015} shows an easy way to implement the technique using the \texttt{VGAM} package in \texttt{R}.  The technique works in two stages.  First, use generalized linear models with a positive Bernoulli family to estimate the the coefficients.  This allows us to obtain fitted values for the probability that each individual in the dataset is missing.  Second, use those fitted values in the Horvitz-Thompson estimator \citep{horvitz_generalization_1952} to estimate the population size, N.  One could take a Bayesian approach to the logistic regression and assign prior distributions to the parameters and estimate the population in a similar way.  We derive equations for finding the maximum a posteriori (MAP) estimate in  Appendix \ref{Sec:conditionalmaximumlike} using gradient ascent. 

\subsubsection{Proposed Extension 1: Nonparametric Distributions}

We propose an alternative approach which is to specify a flexible distribution to the covariates such as a Dirichlet process mixture of normal distributions.  Using stick-breaking priors \citep{ishwaran_gibbs_2001}, this nonparametric approach fits a potentially infinite number of multivariate normal distributions to the data.  The implementation is adapted from \cite{gelman_bayesian_2014}.  This model introduces a latent variable, $z$, which is a latent parameter determining the mean and covariance matrix from which the observed covariate, $\boldsymbol{x_i}$, is drawn. The generative process for each observation's covariate can be summarized,

\begin{align}
\boldsymbol{x_i}|z_i \stackrel{ind}{\sim} & \text{ MVNormal}(\boldsymbol{\mu_k},\boldsymbol{\Sigma_k}) \hspace{5px} \text{for } i=1,...,N\\
z_i \stackrel{iid}{\sim} & \text{ Discrete}(\{1,2,...\},(\pi_1,\pi_2,...)) \hspace{5px} \text{for } i=1,...,N.
\end{align}

Using this generative scheme adds five additional sampling stages to the algorithm presented in subsection \ref{Sec:BLRCRmodel}. The full details including the prior specifications are provided in Appendix \ref{DPnormalmix}. While the number of latent classes is infinite, we approximate this problem by truncating the number of latent classes, $K^*$, and solving this finite-dimensional problem. This upper bound, $K^*$, should not be thought of as a parameter as it should have no impact on the estimation process as long as the value is set sufficiently large enough. It should be noted that the normal distribution specified in subsection \ref{sec:normaldistributioncovariate} is a special case of this specification where $K^*=1$.

\subsubsection{Future Work: Dirichlet Process}

In the previous subsections, we considered a single multivariate normal distribution but also an infinite mixture of normal distributions with membership determined through a stick-breaking process.  We will show in section \ref{Sec:simulations} that the assumption of a single normal may lead to bias in the estimation process when the actual distribution is not normal.  The infinite mixture of normal distributions tends to perform better, but struggles when the distribution is discrete or is far from normally distributed.  An alternative approach would be using the empirical distribution of the covariate.  This has a similar intuition as the mechanic behind the Horvitz-Thompson estimator in the frequentist approach where each covariate essentially gets magnified based on its estimated probability of being missing.  This approach could be made more general by using Dirichlet process mixtures.  As shown in \cite{gelman_bayesian_2014}, a sufficiently large concentration parameter, $\alpha \rightarrow \infty$, converges to the empirical distribution.

